# MM811 Assignment Report
Name: Xinyao Sun
***
## 1.Description of Dataset
he dataset is generated from my [MM804 term projects](https://github.com/Lucklyric/Fall2015MM804Project/tree/For_Deep_Learning_MM811_AS3) Matlab program. I added ["outputTrainningFile.m"](https://github.com/Lucklyric/Fall2015MM804Project/blob/For_Deep_Learning_MM811_AS3/MatLabScripts/outputTrainningFile.m) on branch "For MM811 AS3" to make it able to generate the data file for this assignment.

Each line in the data set includes a set of hands spatial information read from a [Leaps Motion Sensor](https://www.leapmotion.com/) at one frame and is followed by the label (0 or 1) indicates if this sensor returns the most accurate pose estimation among a sensor array at this timestamp. 

Hand motion data is recorded by rotating an artificial hand in the middle of two sensors.  The detail of experiments environment is described in above report. Moreover, each dataset is generated by once recording.

In order to generate the data file for this assignment

`````
git clone -b For_Deep_Learning_MM811_AS3 https://github.com/Lucklyric/Fall2015MM804Project.git
`````

Run the "outputTrainningFile.m" under "MatLabScripts" directory in your MatLab

## 2.Deep Learning Problem
The high-level purpose is for multi-sensors hand motion data fusion to deal with the vision-based sensors' occlusion problem, you can link [here](https://drive.google.com/file/d/0B0LsW1CrvC0RODNtNHFfdWdLcGM/view?usp=sharing) to read the report from my MM804 course project.( In previous work I used SVM to build the classifier. )

The low-level task for using the deep learning is that I want to have a classifier(pre-trained) to intelligently determine which sensor contributes the most accurate hand pose estimation based on a set of selected hands information(feature vector) read from a sensor array (two sensors here).

## 3.Inputs of the model
Inputs of the model is a feature vector with 11 floating point value as follows:
* Three (X, Y, and Z) values of the palm position relative
to the sensor.
* Three (X, Y, and Z) values of the palm plane normal
relative to the sensor.
* Three (X, Y, and Z) values of the forward direction of
the hand.
* The dot product between the palm normal and the direction
the sensor is facing.
* Sensor confidence estimation. The value generated by
an internal proprietary algorithm of the Leap motion
API that is a floating number between 0 and 1.

## 4.Output of the model
The output of the model is 0 or 1:
* 0: This sensor is classified as the sensor that generates worse pose estimation
* 1: This sensor is classified as the sensor that generates better pose estimation

## 5.Performance of selected differnt architecture
I used two datasets to evaluate the performance of different architectures. Two datasets mean there are two independent recording. I used one with 90% split for training and valid and 10% split for testing. Furthermore, in order to measure the condition of overfitting, I used the other dataset to test the trained classifier.


Three models with different layer structures from simple to complex.
All of them has these common settings:

| Arguments        | Value    |    
| ------------- |:-------------:|
| Hidden layer activation              | rectified linear      | 
| Output layer activation          | softmax      |  
|Loss function    |CrossEntropy   |
|momentum | 0.9 |
| Trainers    |layerwise    |

### 5.1.Results are shown as follows:

***
Layers architecture:
* [11,20,2]

Outputs:

| Test Strategy        | TP    |    TN|FP|FN|
| ------------- |:-------------:|:----------:|:----------:|:----------:|
|10% split  | 262 | 266 | 55  |37 |
|Other dataset| 3432| 2968| 1346|882|


***
Layers architecture:
* [11,1000,2]

Outputs:

| Test Strategy        | TP    |    TN|FP|FN|
| ------------- |:-------------:|:----------:|:----------:|:----------:|
|10% split  | 269 | 296 | 27  |28 |
|Other dataset| 3351| 3112| 1202|963|


***
Layers architecture:
* [11,50,500,500,200,50,2]

Outputs:

| Test Strategy        | TP    |    TN|FP|FN|
| ------------- |:-------------:|:----------:|:----------:|:----------:|
|10% split  | 285 | 303 | 16  |16 |
|Other dataset| 3459| 2664| 1650|855|

***
### 5.2.Extra

In order to explore more, here is one sample that adding "regularization" methods.
***
Layers architecture:
* [11,50,500,500,200,50,2]

Hidden-Dropout: 
* 0.5
Outputs:

| Test Strategy        | TP    |    TN|FP|FN|
| ------------- |:-------------:|:----------:|:----------:|:----------:|
|10% split  | 282 | 184 | 146  |8 |
|Other dataset| 3979| 2378| 1936|335|

***
## 6.Discussion
Overall, I tested lots of combination of layers structure, activation and loss functions, the default setting with rectified linear activations on hidden layers and softmax activation on output layer in theanets almost gives best results.
 
 According to the above selected models, experimental results show that more complex deep learning architecture generate more accurate classifier result than simple one a small part of the same dataset (90% for training and valid and 10% for test). if we focus on 10% split test strategy, the number of nodes of the model 1 and model 2's first hidden layer is increased from 20 to 1000, which results in the classified accuracy improved from 85% to 91%. Moreover, after extended the depth of the Neurol network model (model 3), the accuracy is increased to 94%. However, when I used a larger another new dataset, which is not recorded at the same time as the training dataset.  The model with more complex architecture leads slightly worse results. The too deep neural network may cause the overfitting problem. Then I tried with the `dropout` technique on the hidden layers with a value of 0.5, it improved the results a bit but still not good.   
 
Here are some ways I expect to improve the classifier's results.
* Using larger training dataset
* Add one pos-classifier procedure:
  * Each frame I have two feature vector from two sensors. 
  * Use the DNN model with `predict_proba()` function to generate the estimated probability for both sensors.
  * Lable the sensor with a higher probability with 1 and the other with 0 as the final classify result.
* Combine two sensors 11 features (at each frame) to a larger feature vector with size 22, then use this new feature vector as the input to train the DNN.

## 7.Summary
In this homework, I built a deep learning model to intelligently select the sensor with more accurate pose estimation at each time step, based on a subset of the underlying pose estimation data from each sensor.  According to the experiments results, more complex DNN architecture r gives around 94% accuracy but leads around 73% accuracy on the larger new dataset. As the future work, I want to try more regularization methods and the possible ways as I mentioned before to see how well I can improve the final results.
